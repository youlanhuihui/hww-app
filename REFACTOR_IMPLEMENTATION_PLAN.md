# HWW 项目重构实施计划

## 文档说明

- **目标**：在保留 Supabase 后端的前提下，彻底重构代码，使项目可稳定运行，并将 AI 能力全部切换为「用户本机部署的大模型」。
- **约束**：后端继续使用 Supabase；前端需提供「本地大模型部署指引」与「连接状态看板」；**本文档仅作实施计划，不执行任何代码修改**，待确认后再分阶段执行。

---

## 一、现状与目标架构

### 1.1 当前问题摘要

| 问题类型 | 表现 | 根因（推断） |
|----------|------|----------------|
| 无法稳定运行 | 构建/运行报错、功能不可用 | 多模块耦合、LLM 依赖云厂商 Key、接口不统一 |
| AI 依赖云端 | 使用 DashScope/Coze 等，需 API Key | 与「本地大模型」目标不符 |
| 无本地模型入口 | 无安装与连接引导 | 产品未设计本地模型使用路径 |
| 服务端调用 LLM | task-engine、agent、im-task 等在服务端调 LLM | 服务端无法访问用户本机 localhost |

### 1.2 目标架构原则

1. **服务端（Next.js 部署在服务器）**
   - 只做：认证、Supabase 读写、会话/任务元数据、非 LLM 的业务逻辑。
   - **不再**直接调用任何大模型 API（不调用 DashScope/Coze/OpenAI，也不代理到用户本机）。

2. **用户本机**
   - 用户自行安装并运行「本地大模型服务」（如 Ollama、或兼容 OpenAI API 的本地服务）。
   - 仅**浏览器（前端）**向用户配置的「本地模型地址」发起请求（如 `http://localhost:11434` 或本机局域网 IP）。

3. **前端**
   - 提供「本地大模型」的：**部署指引**、**连接配置**、**连接状态看板**。
   - 所有对话、任务规划/执行中涉及的 LLM 调用，由前端或前端驱动的流程发起，请求发往用户本机地址。

### 1.3 目标架构图（逻辑）

```
┌─────────────────────────────────────────────────────────────────┐
│  用户浏览器（前端）                                                │
│  - 部署指引 + 连接配置 + 状态看板                                   │
│  - Chat / 任务输入                                                │
│  - 前端 LLM 客户端 → 仅请求「用户配置的本地模型 URL」                │
└───────────────────────┬─────────────────────────────────────────┘
                        │ fetch(用户本机 URL，如 http://localhost:11434)
                        ▼
┌─────────────────────────────────────────────────────────────────┐
│  用户本机                                                         │
│  - Ollama / 其他 OpenAI 兼容 API（用户自行安装与启动）              │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│  服务器（Next.js）                                                │
│  - 页面、静态资源、API（仅 Supabase + 业务元数据，无 LLM 调用）      │
└─────────────────────────────────────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────────────────┐
│  Supabase（后端）                                                 │
│  - Auth、DB、Storage                                              │
└─────────────────────────────────────────────────────────────────┘
```

---

## 二、实施阶段总览

| 阶段 | 名称 | 目标 | 预估工作量 |
|------|------|------|------------|
| **Phase 0** | 可运行基线 | 修复构建与最小可运行路径，不改业务逻辑 | 中 |
| **Phase 1** | 数据与认证 | 统一 Supabase 客户端与 Auth，保证读写可靠 | 中 |
| **Phase 2** | 本地模型配置与看板 | 前端：部署指引 + 配置 + 状态检测 | 中 |
| **Phase 3** | 前端 LLM 与对话 | 前端统一调用本地模型，Chat 可用 | 大 |
| **Phase 4** | 任务引擎与技能 | 任务引擎改为「前端驱动 + 本地 LLM」或精简版 | 大 |
| **Phase 5** | 清理与收尾 | 移除云端 LLM、废弃代码、文档与脚本 | 中 |

---

## 三、Phase 0：可运行基线

**目标**：项目能 `pnpm install`、`pnpm build`、`pnpm dev` 通过，首页可打开，无阻塞性报错。

### 3.1 构建与依赖

- 检查并固定 Node 版本（如 20 LTS），在 `package.json` 或文档中注明。
- 解决所有 `pnpm build` 的 TypeScript/ESLint 错误（必要时先收紧 `next.config` 中的 `ignoreBuildErrors` 再逐步修）。
- 确保 `next.config.mjs`、`tsconfig.json`、`middleware.ts` 无冲突（如 matcher 与 API 路由）。

### 3.2 入口与路由

- 保证根布局 `app/layout.tsx` 能正常渲染，不依赖未初始化的环境变量（Supabase URL/Key 可读且有效或优雅降级）。
- 检查 `app/page.tsx` 与重定向逻辑，确保访问 `/` 有明确行为（如进入主应用或引导页）。
- 修复 `middleware` 中可能阻止 API 或关键路由的 matcher/逻辑。

### 3.3 环境变量

- 明确必需变量：`NEXT_PUBLIC_SUPABASE_URL`、`NEXT_PUBLIC_SUPABASE_ANON_KEY`。
- 提供 `.env.example`，并说明本地开发时如何创建 `.env.local`。
- 对未配置的 Supabase 做降级（如仅隐藏需要登录的功能，不直接抛错导致白屏）。

### 3.4 交付物

- 项目可一键安装、构建、本地运行。
- 简短「本地运行说明」写入 README 或 `docs/DEV.md`。

---

## 四、Phase 1：数据与认证统一

**目标**：全项目统一使用 Supabase 客户端与类型，Auth 流程清晰、可复用。

### 4.1 Supabase 客户端

- **服务端**：在 `lib/supabase` 中提供唯一的「服务端 createClient」方法（如基于 cookie 的 `createServerClient`），供所有 API 路由与 Server Components 使用；避免各 API 内重复 `createClient(supabaseUrl, serviceKey)` 且 key 来源不一致。
- **浏览器**：保留并统一 `createBrowserClient`/`getClient()`，确保与 SSR 的 cookie 策略一致。
- 所有 API 路由中涉及 Supabase 的，改为从 `@/lib/supabase` 的同一套工厂方法获取客户端。

### 4.2 类型与迁移

- 以现有 `supabase/migrations` 为基准，确保 `lib/supabase/types.ts`（或生成的类型）与当前库表一致；若不一致，以迁移为准重新生成或手调类型。
- 关键表：`profiles`、`skills`、任务相关表（如 `task_sessions`、`task_plans`）、IM/订单等，在类型中均有体现且无冲突。

### 4.3 认证流程

- 登录/注册/登出统一走 Supabase Auth，前端只调用 `supabase.auth.signIn/signUp/signOut` 等，不引入额外自定义 token 逻辑。
- 需要「当前用户」的 API：从服务端 Supabase 客户端取 `getUser()`（或等价），未登录时返回 401 或明确错误，不在服务端使用 anon key 冒充用户。

### 4.4 交付物

- 所有 Supabase 访问收敛到 `lib/supabase`。
- 登录/注册/登出可完成一次完整流程，且 API 能正确识别当前用户。

---

## 五、Phase 2：本地大模型「部署 + 连接」看板

**目标**：在前端提供「如何在本机部署大模型」的指引，以及「配置连接 + 实时状态」的看板。

### 5.1 部署指引（静态内容 + 可选链接）

- 新建页面或区块：「本地大模型部署指南」。
  - 推荐方案：**Ollama**（跨平台、易安装）。
  - 内容：下载地址、安装步骤（Windows/macOS/Linux 简要步骤）、启动方式、默认端口（如 11434）。
  - 可选：补充「其他兼容 OpenAI API 的本地服务」说明（如 LM Studio、LocalAI），并说明「在设置中填写其 API 地址即可」。
- 可放在：设置页中的「模型」子页、或独立「模型 / 本地模型」页，并在主导航或首页显眼处入口。

### 5.2 连接配置（前端）

- **存储**：本地模型 base URL（及可选模型名）存于 **前端**：
  - 优先：**Supabase**（如 `profiles` 扩展字段或单独表 `user_settings`），键如 `local_model_base_url`、`local_model_name`，便于多设备同步。
  - 备选：`localStorage`，键名统一（如 `hww_local_model_url`），仅当未登录或表未就绪时使用。
- **UI**：
  - 输入框：Base URL（默认 `http://localhost:11434`，Ollama 默认）。
  - 可选：模型名（如 `llama3.2`），若本地 API 支持多模型。
  - 「测试连接」按钮：前端对该 URL 发起健康检查请求（如 GET `/api/tags` for Ollama 或 GET `/v1/models` for OpenAI 兼容），根据结果提示成功/失败。

### 5.3 连接状态看板

- **状态**：在「模型/设置」页或全局侧边/顶栏展示：
  - **未配置**：未填写 base URL。
  - **检查中**：正在请求健康检查。
  - **已连接**：最近一次检查成功；可展示模型名或「Ollama 就绪」等简短信息。
  - **连接失败**：最近一次检查失败；可展示错误原因（如「无法访问，请确认 Ollama 已启动」）。
- **刷新**：支持手动「重新检测」；可选：进入 Chat 页时自动检测一次。
- 不依赖服务端参与「检测」逻辑（检测由前端直接请求用户本机 URL 完成）。

### 5.4 交付物

- 用户能按指引在本机安装并启动 Ollama（或兼容服务）。
- 用户能在前端配置 base URL 并看到明确的「已连接/未连接」状态。

---

## 六、Phase 3：前端 LLM 与对话（Chat）

**目标**：所有对话类请求由前端直接发往用户配置的本地模型，服务端不再参与 LLM 调用。

### 6.1 前端 LLM 客户端（新建）

- 新建模块，如 `lib/llm/local-client.ts`（或 `lib/llm/browser-client.ts`），**仅用于浏览器**：
  - 输入：base URL（从 Phase 2 的配置读取）、可选 model 名。
  - 实现：`chat(messages)`、`chatStream(messages)`，内部使用 `fetch` 调用用户本机 URL。
  - 协议：优先 **Ollama 原生**（如 POST `/api/chat`），或 **OpenAI 兼容**（POST `/v1/chat/completions`），在实现中二选一或做简单分支（推荐先 Ollama，再兼容 OpenAI 格式）。
- 该模块不依赖 `getDefaultClient()`、不依赖服务端环境变量（如 DASHSCOPE_API_KEY），不被打包进服务端 bundle（可通过动态 import 或仅在被浏览器使用的组件中引用）。

### 6.2 对话流程改造

- **当前**：前端可能通过 `/api/agent` 或类似接口发消息，服务端再调 Coze/LLM。
- **目标**：
  - 前端直接使用上述「前端 LLM 客户端」向本地模型发请求；流式响应用 `chatStream`，在 UI 中逐字/逐块渲染。
  - 若需要「持久化对话」：前端在收到完整回复后，调用**自有 API**（仅写 Supabase），保存会话与消息到 DB（不经过服务端 LLM）。
- 涉及的前端页面：主对话页（如 Chat 页、首页对话框）改为：输入 → 前端 LLM 客户端 → 本地模型 → 流式/非流式展示 → 可选保存到 Supabase。

### 6.3 服务端 API 瘦身

- `/api/agent`、`/api/coze/*` 等原先用于「服务端调 LLM/Coze」的接口：
  - 要么移除，要么改为「仅写会话/消息到 Supabase、或返回会话历史」等非 LLM 逻辑；不再在服务端调用任何第三方或本地 LLM。
- 保留的 API：如「保存对话」「获取历史会话列表」「获取某会话消息」等，仅做 Supabase 读写与鉴权。

### 6.4 交付物

- 用户配置好本地模型后，在 Chat 页输入并发送，对话完全由本机模型响应，且可在 UI 中看到流式输出。
- 可选：对话可持久化到 Supabase（由前端调 API 写入）。

---

## 七、Phase 4：任务引擎与技能（本地 LLM 驱动）

**目标**：任务规划/执行中的 LLM 调用改为「前端发起、请求用户本机模型」，服务端只做编排与持久化（或精简为仅持久化）。

### 7.1 任务引擎调用链现状

- 当前：`app/api/task-engine/route.ts` 等使用 `TaskEngineController`，内部使用 `getDefaultClient()`、`createLLMClient()` 等在**服务端**调 LLM（DashScope/Coze/Ollama 等）。
- 问题：服务端无法访问用户本机，因此这些调用无法指向「用户本机模型」。

### 7.2 改造方向（二选一或分步）

**方案 A：任务引擎前端化（推荐）**

- 将「规划 + 单步执行」中需要 LLM 的部分移到前端：
  - 前端持有「任务引擎」的轻量版（或复用现有逻辑但运行在浏览器中），在需要生成计划、生成子任务、执行某步时，由前端调用 Phase 3 的「前端 LLM 客户端」请求本地模型。
  - 服务端 API：只负责「创建会话、保存计划、保存执行结果、流式事件转发」等；或简化为 REST：前端把「当前步骤的输入与结果」提交给服务端，服务端只写库。
- 优点：所有 LLM 调用都在用户本机内完成，架构清晰。缺点：需将部分 `lib/task-engine` 逻辑改为可在浏览器运行（或拆出「纯函数 + 请求接口」）。

**方案 B：服务端编排 + 客户端执行**

- 服务端只生成「步骤描述」与参数，不调用 LLM；具体「每步的 prompt → 本地模型 → 结果」由前端执行，前端再把结果回传服务端。
- 服务端 API：返回「下一步要执行的 prompt/参数」；前端调用本地模型得到结果后，再 POST 结果到服务端，服务端写入 DB 并返回下一步或结束。
- 优点：服务端逻辑简单。缺点：交互轮次多，需定义好「步骤协议」。

建议：**先实现方案 B**（协议简单、改动集中在前端 + 少量 API），待稳定后再考虑将更多规划逻辑下放到前端的方案 A。

### 7.3 技能市场与执行

- 技能执行若依赖 LLM：同样改为「由前端请求本地模型」或「由前端调用本地已注册技能」；服务端只做技能元数据、安装状态、权限等读写。
- 若技能为「纯工具」（无 LLM）：可保留在服务端或前端执行，视实现而定；与「本地模型」无冲突。

### 7.4 交付物

- 用户创建/执行任务时，规划与执行中的 LLM 调用全部来自其本机模型；服务端不再调 LLM。
- 任务状态、计划、结果仍持久化在 Supabase。

---

## 八、Phase 5：清理与收尾

**目标**：移除对云端 LLM 的依赖，删除废弃代码，统一配置与文档。

### 8.1 移除或替换的代码

- **lib/llm**（服务端用）：
  - 保留类型定义、以及「供服务端做简单校验/占位」的接口（若有）；删除或禁用所有在服务端实际发起 LLM 请求的代码（如 `getDefaultClient()` 在服务端的实际调用、DashScope/Coze provider 的请求逻辑）。
  - 或：将 `lib/llm` 重命名为 `lib/llm-server` 并标记为 deprecated，新逻辑全部走 `lib/llm/local-client.ts`（浏览器）。
- **lib/agents**：若内部调用了云端 LLM，改为调用「由前端传入的本地模型客户端」或移除；或保留为「仅工具类 Agent」，由前端在调用工具时再请求本地模型。
- **lib/services/coze-service.ts**、**lib/services/universal-agent.ts**：去掉对 Coze 云 API 的依赖；可改为「仅意图识别 + 路由到前端执行」或删除，由 Phase 3/4 的新流程替代。
- **环境变量**：从文档和 `.env.example` 中移除 `DASHSCOPE_API_KEY`、`COZE_*` 等；保留 `NEXT_PUBLIC_SUPABASE_*` 等。

### 8.2 配置与文档

- README：说明「本项目使用本地大模型」、如何安装 Ollama、如何在前端配置连接；列出必需环境变量（仅 Supabase 相关）。
- 可选：`docs/ARCHITECTURE.md` 简述「前端 → 本地模型」「服务端 → Supabase」的边界。

### 8.3 测试与回归

- 关键路径：安装依赖 → 配置 Supabase → 配置本地模型 URL → 打开 Chat → 发送消息 → 收到本地模型回复。
- 任务流程：创建任务 → 规划/执行依赖本地模型 → 结果写入 Supabase 并可查看。
- 不再依赖任何云端 LLM API Key。

### 8.4 交付物

- 代码库中无对 DashScope/Coze 等云端 LLM 的主动调用；本地模型配置与状态仅在前端可见。
- README 与可选架构文档更新完毕；项目可仅凭 Supabase + 本地模型完整跑通。

---

## 九、风险与依赖

| 风险 | 缓解 |
|------|------|
| 前端直连用户本机存在 CORS | 本地模型（Ollama 等）需允许浏览器来源；在部署指引中说明「若 CORS 报错，请配置本地服务允许当前站点」；必要时前端通过 Next 同源 API 做代理（仅代理到用户配置的 URL，且仅开发/内网使用）。 |
| 任务引擎逻辑复杂，迁移易漏 | 先采用方案 B（服务端只编排，前端执行 LLM），减少对现有 task-engine 的改动面；再逐步把可迁移逻辑下放到前端。 |
| Supabase 表结构与现有代码不一致 | Phase 1 以迁移文件为准统一类型与客户端；必要时增加一次性数据修复脚本。 |

---

## 十、执行顺序与节奏建议

1. **先做 Phase 0**：确保能跑起来，再谈功能。
2. **再做 Phase 1**：所有读写与认证都走 Supabase 统一入口，避免后续改 API 时到处找客户端。
3. **Phase 2 与 Phase 3 可部分并行**：先做「配置 + 状态看板」（Phase 2），再做「前端 LLM 客户端 + Chat」（Phase 3）；Phase 3 依赖 Phase 2 的配置读取。
4. **Phase 4** 在 Phase 3 稳定后进行；**Phase 5** 在 Phase 4 主流程打通后做清理。

---

## 十一、总结

- **后端**：继续用 Supabase 管理与对接数据；服务端**不再**调用任何大模型。
- **前端**：提供本地大模型的**部署方法**与**连接配置 + 状态看板**，所有 AI 能力通过**前端请求用户本机部署的模型**实现。
- **实施顺序**：可运行基线 → 数据与认证统一 → 本地模型看板 → 前端对话 → 任务引擎/技能改造 → 清理与文档。

